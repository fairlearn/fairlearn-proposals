{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, Y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_raw[['Sex','Race']]\n",
    "X = X_raw.drop(labels=['Sex', 'Race'],axis = 1)\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, \n",
    "                                                    Y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "# Work around indexing issue\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "# Improve labels\n",
    "A_test.Sex.loc[(A_test['Sex'] == 0)] = 'female'\n",
    "A_test.Sex.loc[(A_test['Sex'] == 1)] = 'male'\n",
    "\n",
    "\n",
    "A_test.Race.loc[(A_test['Race'] == 0)] = 'Amer-Indian-Eskimo'\n",
    "A_test.Race.loc[(A_test['Race'] == 1)] = 'Asian-Pac-Islander'\n",
    "A_test.Race.loc[(A_test['Race'] == 2)] = 'Black'\n",
    "A_test.Race.loc[(A_test['Race'] == 3)] = 'Other'\n",
    "A_test.Race.loc[(A_test['Race'] == 4)] = 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "lr_predictor.fit(X_train, Y_train)\n",
    "Y_pred_lr = lr_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictor = svm.SVC()\n",
    "\n",
    "svm_predictor.fit(X_train, Y_train)\n",
    "Y_pred_svm = svm_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score\n",
    "from fairlearn.metrics import group_summary, make_derived_metric, difference_from_summary, make_metric_group_summary\n",
    "from fairlearn.metrics import demographic_parity_difference, balanced_accuracy_score_group_min\n",
    "from fairlearn.metrics import false_negative_rate, false_positive_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report one disaggregated metric in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "bunch = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = pd.Series(bunch.by_group)\n",
    "frame_o = pd.Series({**bunch.by_group, 'overall': bunch.overall})\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = result.by_group\n",
    "frame_o = result.to_df() # Throw if there is a group called 'overall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report several disaggregated metrics in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "bunch1 = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "bunch2 = group_summary(f1_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = pd.DataFrame({\n",
    "   'accuracy': bunch1.by_group, 'f1': bunch2.by_group})\n",
    "frame_o = pd.DataFrame({\n",
    "   'accuracy': {**bunch1.by_group, 'overall': bunch1.overall},\n",
    "   'f1': {**bunch2.by_group, 'overall': bunch2.overall}})\n",
    "\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric({ 'accuracy':accuracy_score, 'f1':f1_score}, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = result.by_group\n",
    "frame_o = result.to_df() # Throw if there is a group called 'overall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report metrics for intersecting sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "sf = A_test['Race']+'-'+A_test['Sex'] # User builds new column manually\n",
    "\n",
    "bunch = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=sf)\n",
    "frame = pd.Series(bunch.by_group)\n",
    "frame_o = pd.Series({**bunch.by_group, 'overall': bunch.overall})\n",
    "\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric(accuracy_score, Y_test, Y_pred_lr, sensitive_features=[A['Race'], A['Sex']])\n",
    "frame = result.by_group # Will have a MultiIndex built from the two sensitive feature columns\n",
    "frame_o = result.to_def() # Not sure how to handle adding the extra 'overall' row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report several performance and fairness metrics of several models in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "fb_s = lambda y_t, y_p: fbeta_score(y_t, y_p, beta=0.5)\n",
    "custom_difference1 = make_derived_metric(\n",
    "    difference_from_summary,\n",
    "    make_metric_group_summary(fb_s))\n",
    "\n",
    "def custom_difference2(y_true, y_pred, sensitive_features):\n",
    "    bunch = group_summary(fbeta_score, y_true, y_pred, sensitive_features=sensitive_features, beta=0.5)\n",
    "    frame = pd.Series(bunch.by_group)\n",
    "    return (frame-frame['White']).min()\n",
    "\n",
    "fairness_metrics = {\n",
    "    'Custom difference 1': custom_difference1,\n",
    "    'Custom difference 2': custom_difference2,\n",
    "    'Demographic parity difference': demographic_parity_difference,\n",
    "    'Worst-case balanced accuracy': balanced_accuracy_score_group_min}\n",
    "performance_metrics = {\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate}\n",
    "predictions_by_estimator = {\n",
    "    'logreg': Y_pred_lr,\n",
    "    'svm': Y_pred_svm}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for pred_key, y_pred in predictions_by_estimator.items():\n",
    "    for fairm_key, fairm in fairness_metrics.items():\n",
    "        df.loc[fairm_key, pred_key] = fairm(Y_test, y_pred, sensitive_features=A_test['Race'])\n",
    "    for perfm_key, perfm in performance_metrics.items():\n",
    "        df.loc[perfm_key, pred_key] = perfm(Y_test, y_pred)\n",
    "        \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "custom_difference1 = make_derived_metric('difference', fbeta_score, parms={'beta', 0.5})\n",
    "\n",
    "def custom_difference2(y_true, y_pred, sensitive_features):\n",
    "    tmp = GroupedMetric(fbeta_score, y_true, y_pred, sensitive_features=sensitive_features, parms={'beta':0.5})\n",
    "    return tmp.differences(relative_to='group', group='White', aggregate='min')\n",
    "\n",
    "# The remainder as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fairness-performance raster plot of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "my_disparity_metric=custom_difference1\n",
    "my_performance_metric=false_positive_rate\n",
    "\n",
    "xs = [my_performance_metric(Y_test, y_pred) for y_pred in predictions_by_estimator.values()]\n",
    "ys = [my_disparity_metric(Y_test, y_pred, sensitive_features=A_test['Race']) \n",
    "      for y_pred in predictions_by_estimator.values()]\n",
    "\n",
    "plt.scatter(xs,ys)\n",
    "plt.xlabel('Performance Metric')\n",
    "plt.ylabel('Disparity Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "\n",
    "# Would also reuse the definition of custom_difference1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sklearn.model_selection.cross_validate\n",
    "\n",
    "Use demographic parity and precision score as the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "precision_scorer = make_scorer(precision_score)\n",
    "\n",
    "y_t = pd.Series(Y_test)\n",
    "def dpd_wrapper(y_t, y_p, sensitive_features):\n",
    "    # We need to slice up the sensitive feature to match y_t and y_p\n",
    "    # See Adrin's reply to:\n",
    "    # https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function\n",
    "    sf_slice = sensitive_features.loc[y_t.index.values].values.reshape(-1)\n",
    "    return demographic_parity_difference(y_t, y_p, sensitive_features=sf_slice)\n",
    "dp_scorer = make_scorer(dpd_wrapper, sensitive_features=A_test['Race'])\n",
    "\n",
    "scoring = {'prec':precision_scorer, 'dp':dp_scorer}\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, X_test, y_t, scoring=scoring)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "\n",
    "# Would be the same, until Adrin's SLEP/PR are accepted to help with input slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 7: Run GridSearchCV\n",
    "\n",
    "Use demographic parity and precision score where the goal is to find the lowest-error model whose demographic parity is <= 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "scoring = {'prec':precision_scorer, 'dp':dp_scorer}\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "gscv = GridSearchCV(clf, param_grid=param_grid, scoring=scoring, refit='prec', verbose=1)\n",
    "gscv.fit(X_test, y_t)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")  \n",
    "print(gscv.best_params_)\n",
    "print(\"Best score:\", gscv.best_score_)\n",
    "print()\n",
    "print(\"Overall results\")\n",
    "print(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "\n",
    "# Would be the same, until Adrin's SLEP/PR are accepted to help with input slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
