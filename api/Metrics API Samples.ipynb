{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, Y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_raw[['Sex','Race']]\n",
    "X = X_raw.drop(labels=['Sex', 'Race'],axis = 1)\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, \n",
    "                                                    Y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "# Work around indexing issue\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "# Improve labels\n",
    "A_test.Sex.loc[(A_test['Sex'] == 0)] = 'female'\n",
    "A_test.Sex.loc[(A_test['Sex'] == 1)] = 'male'\n",
    "\n",
    "\n",
    "A_test.Race.loc[(A_test['Race'] == 0)] = 'Amer-Indian-Eskimo'\n",
    "A_test.Race.loc[(A_test['Race'] == 1)] = 'Asian-Pac-Islander'\n",
    "A_test.Race.loc[(A_test['Race'] == 2)] = 'Black'\n",
    "A_test.Race.loc[(A_test['Race'] == 3)] = 'Other'\n",
    "A_test.Race.loc[(A_test['Race'] == 4)] = 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "lr_predictor.fit(X_train, Y_train)\n",
    "Y_pred_lr = lr_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictor = svm.SVC()\n",
    "\n",
    "svm_predictor.fit(X_train, Y_train)\n",
    "Y_pred_svm = svm_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score\n",
    "from fairlearn.metrics import group_summary, make_derived_metric, difference_from_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report one disaggregated metric in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amer-Indian-Eskimo    0.923077\n",
      "Asian-Pac-Islander    0.840796\n",
      "Black                 0.914826\n",
      "Other                 0.851064\n",
      "White                 0.826492\n",
      "dtype: float64\n",
      "=======================\n",
      "Amer-Indian-Eskimo    0.923077\n",
      "Asian-Pac-Islander    0.840796\n",
      "Black                 0.914826\n",
      "Other                 0.851064\n",
      "White                 0.826492\n",
      "overall               0.836481\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Current\n",
    "bunch = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = pd.Series(bunch.by_group)\n",
    "frame_o = pd.Series({**bunch.by_group, 'overall': bunch.overall})\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6513\n"
     ]
    }
   ],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = result.by_group\n",
    "frame_o = result.to_df() # Throw if there is a group called 'overall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report several disaggregated metrics in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    accuracy        f1\n",
      "Amer-Indian-Eskimo  0.923077  0.666667\n",
      "Asian-Pac-Islander  0.840796  0.652174\n",
      "Black               0.914826  0.550000\n",
      "Other               0.851064  0.000000\n",
      "White               0.826492  0.612800\n",
      "=======================\n",
      "                    accuracy        f1\n",
      "Amer-Indian-Eskimo  0.923077  0.666667\n",
      "Asian-Pac-Islander  0.840796  0.652174\n",
      "Black               0.914826  0.550000\n",
      "Other               0.851064  0.000000\n",
      "White               0.826492  0.612800\n",
      "overall             0.836481  0.610033\n"
     ]
    }
   ],
   "source": [
    "# Current\n",
    "bunch1 = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "bunch2 = group_summary(f1_score, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = pd.DataFrame({\n",
    "   'accuracy': bunch1.by_group, 'f1': bunch2.by_group})\n",
    "frame_o = pd.DataFrame({\n",
    "   'accuracy': {**bunch1.by_group, 'overall': bunch1.overall},\n",
    "   'f1': {**bunch2.by_group, 'overall': bunch2.overall}})\n",
    "\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric({ 'accuracy':accuracy_score, 'f1':f1_score}, Y_test, Y_pred_lr, sensitive_features=A_test['Race'])\n",
    "frame = result.by_group\n",
    "frame_o = result.to_df() # Throw if there is a group called 'overall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report metrics for intersecting sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amer-Indian-Eskimo-female    0.937500\n",
      "Amer-Indian-Eskimo-male      0.916667\n",
      "Asian-Pac-Islander-female    0.879310\n",
      "Asian-Pac-Islander-male      0.825175\n",
      "Black-female                 0.962382\n",
      "Black-male                   0.866667\n",
      "Other-female                 0.909091\n",
      "Other-male                   0.833333\n",
      "White-female                 0.917824\n",
      "White-male                   0.785510\n",
      "dtype: float64\n",
      "=======================\n",
      "Amer-Indian-Eskimo-female    0.937500\n",
      "Amer-Indian-Eskimo-male      0.916667\n",
      "Asian-Pac-Islander-female    0.879310\n",
      "Asian-Pac-Islander-male      0.825175\n",
      "Black-female                 0.962382\n",
      "Black-male                   0.866667\n",
      "Other-female                 0.909091\n",
      "Other-male                   0.833333\n",
      "White-female                 0.917824\n",
      "White-male                   0.785510\n",
      "overall                      0.836481\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Current\n",
    "sf = A_test['Race']+'-'+A_test['Sex'] # User builds new column manually\n",
    "\n",
    "bunch = group_summary(accuracy_score, Y_test, Y_pred_lr, sensitive_features=sf)\n",
    "frame = pd.Series(bunch.by_group)\n",
    "frame_o = pd.Series({**bunch.by_group, 'overall': bunch.overall})\n",
    "\n",
    "print(frame)\n",
    "print(\"=======================\")\n",
    "print(frame_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed\n",
    "result = GroupedMetric(accuracy_score, Y_test, Y_pred_lr, sensitive_features=[A['Race'], A['Sex']])\n",
    "frame = result.by_group # Will have a MultiIndex built from the two sensitive feature columns\n",
    "frame_o = result.to_def() # Not sure how to handle adding the extra 'overall' row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report several performance and fairness metrics of several models in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_group_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-3c393d25f724>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m custom_difference1 = make_derived_metric(\n\u001b[0;32m      3\u001b[0m     \u001b[0mdifference_from_summary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     make_group_summary(fbeta_score, beta=0.5))\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcustom_difference2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_group_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# Current\n",
    "custom_difference1 = make_derived_metric(\n",
    "    difference_from_summary,\n",
    "    make_group_summary(fbeta_score, beta=0.5))\n",
    "\n",
    "def custom_difference2(y_true, y_pred, sf):\n",
    "    bunch = group_summary(fbeta_score, y_true, y_pred, sf, beta=0.5)\n",
    "    frame = pd.Series(bunch.by_group)\n",
    "    return (frame-frame['B']).min()\n",
    "\n",
    "fairness_metrics = {\n",
    "    'Custom difference 1': custom_difference,\n",
    "    'Demographic parity difference': demographic_parity_difference,\n",
    "    'Worst-case balanced accuracy': balanced_accuracy_group_min}\n",
    "perfomance_metrics = {\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate}\n",
    "predictions_by_estimator = {\n",
    "    'logreg': y_pred_lr,\n",
    "    'svm': y_pred_svm}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for pred_key, y_pred in predictions_by_estimator.items():\n",
    "    for fairm_key, fairm in fairness_metrics.items():\n",
    "        df.loc[fairm_key, pred_key] = fairm(y_true, y_pred, sf)\n",
    "    for perfm_key, perfm in performance_metrics.items():\n",
    "        df.loc[perfm_key, pred_key] = perfm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
